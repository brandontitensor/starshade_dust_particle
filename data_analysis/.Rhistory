# Load required libraries
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(tidyverse)
library(knitr)
library(rstatix)
library(ggpubr)
library(gridExtra)
library(data.table)
library(cowplot)
library(ggplot2)
library(dplyr)
library(grid)
library(fs)
# Create unified theme
custom_theme <- theme_minimal() +
theme(
plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
plot.subtitle = element_text(size = 10, hjust = 0.5),
axis.title = element_text(size = 10),
axis.text = element_text(size = 8),
legend.position = "bottom",
legend.title = element_text(size = 10),
legend.text = element_text(size = 8),
panel.grid.major = element_line(color = "gray90"),
panel.grid.minor = element_line(color = "gray95")
)
# Define consistent color palette
trial_colors <- c(
"6" = "#E41A1C",  # red
"7" = "#FF7F00",  # orange
"8" = "#377EB8",  # blue
"9" = "#4DAF4A",  # green
"10" = "#984EA3", # purple
"11" = "#CEB180"  # tan
)
# Define consistent line types
line_types <- c(
"Observed" = "solid",
"Modeled" = "dashed",
"Calibration" = "dotted"
)
# Function to load and process a single sample of edge data
load_sample_data <- function(sample_number) {
# Format sample number with leading zeros
sample_str <- sprintf("%02d", sample_number)
# Construct file paths for particles and summary data
particles_path <- sprintf("~/Desktop/College/Research/Dust_Contamination/Trials/Data/Edges/Edge Measurements/Bef_%s minus Aft_%s/Particles Bef_%s minus Aft_%s.csv",
sample_str, sample_str, sample_str, sample_str)
summary_path <- sprintf("~/Desktop/College/Research/Dust_Contamination/Trials/Data/Edges/Edge Measurements/Bef_%s minus Aft_%s/Summary Bef_%s minus Aft_%s_updated.csv",
sample_str, sample_str, sample_str, sample_str)
# Load and preprocess particle data
particles_data <- read.csv(particles_path)
particles_data$Sample <- sample_number
# Load and preprocess summary data
summary_data <- read.csv(summary_path)
summary_data <- summary_data[,-c(8:11)]
names(summary_data) <- make.names(names(summary_data))
summary_data$Count <- as.numeric(gsub("[^0-9.]", "", summary_data$Count))
summary_data$width <- as.numeric(gsub("[^0-9.]", "", summary_data$width))
summary_data <- summary_data %>% drop_na()
summary_data$Sample <- sample_number
# Convert width from pixels to microns
summary_data$width <- summary_data$width * (50/240)
list(particles = particles_data, summary = summary_data)
}
# Load data for samples 36-50 (Trials 6-10, 5 samples each)
edge_data <- map(c(36:55), load_sample_data)
# Combine all particle data
edge_particles_data <- bind_rows(map(edge_data, "particles"))
# Combine all summary data
edge_summary_data <- bind_rows(map(edge_data, "summary"))
# Assign trials to samples (5 samples per trial)
edge_particles_data$Trial <- ceiling((edge_particles_data$Sample - 5) / 5) + 1
edge_summary_data$Trial <- ceiling((edge_summary_data$Sample - 5) / 5) + 1
# Calculate total width for each trial
total_width_by_trial <- edge_summary_data %>%
group_by(Trial) %>%
summarise(Total_Width = sum(width))
# Calculate the normalization factor
max_width <- max(total_width_by_trial$Total_Width)
normalization_factors <- max_width / total_width_by_trial$Total_Width
# Function to load and process surface data
load_data <- function(condition) {
# Function to load and process particle data for a single sample
load_particle_data <- function(trial_number, sample_number, cond) {
tryCatch({
particle_path <- sprintf("~/Desktop/College/Research/Dust_Contamination/Trials/Data/Surfaces/%sTr%dSa%dSurf/Particles_%sTr%dSa%dSurf.csv",
cond, trial_number, sample_number, cond, trial_number, sample_number)
particle_data <- read_csv(particle_path)
particle_data <- particle_data[, 2:(ncol(particle_data)-2)]
particle_data$Sample <- sample_number
particle_data$Trial <- trial_number
return(particle_data)
}, error = function(e) {
message(sprintf("Error loading particle data for %s Trial %d, Sample %d: %s",
cond, trial_number, sample_number, e$message))
return(NULL)
})
}
# Load and process summary data
load_summary_data <- function(trial_number, sample_number, cond) {
tryCatch({
summary_path <- sprintf("~/Desktop/College/Research/Dust_Contamination/Trials/Data/Surfaces/%sTr%dSa%dSurf/Summary_%sTr%dSa%dSurf.csv",
cond, trial_number, sample_number, cond, trial_number, sample_number)
summary_data <- read_csv(summary_path, col_select = c(1, 2, 3, 5))
names(summary_data) <- make.names(names(summary_data))
summary_data <- summary_data %>%
drop_na() %>%
mutate(
Sample = sample_number,
Trial = trial_number,
Slice_Number = as.integer(substr(Slice, nchar(Slice)-6, nchar(Slice)-4))
)
return(summary_data)
}, error = function(e) {
message(sprintf("Error loading summary data for %s Trial %d, Sample %d: %s",
cond, trial_number, sample_number, e$message))
return(NULL)
})
}
# Load data for trials 8-11
surface_particle_data <- map2(rep(8:11, each = 5), rep(1:5, times = 4),
~load_particle_data(.x, .y, condition))
surface_summary_data <- map2(rep(8:11, each = 5), rep(1:5, times = 4),
~load_summary_data(.x, .y, condition))
# Combine all data
surface_particle_data <- bind_rows(surface_particle_data)
surface_summary_data <- bind_rows(surface_summary_data)
# Add cleaning method information
add_cleaning_method <- function(data) {
data %>%
mutate(Cleaning_Method = case_when(
Trial %in% c(2, 3) ~ "IPA rinse",
Trial %in% c(4, 5) ~ "Drag and wipe",
Trial %in% c(6, 7) ~ "First contact",
Trial %in% c(8, 9, 10, 11) ~ "First contact & Drag and wipe",
TRUE ~ NA_character_
))
}
surface_particle_data <- add_cleaning_method(surface_particle_data)
surface_summary_data <- add_cleaning_method(surface_summary_data)
# Calculate imaged areas
image_size <- 600 * 450 # microns^2
trial_areas <- surface_summary_data %>%
group_by(Trial, Cleaning_Method) %>%
summarise(
Total_Images = n(),
Total_Area = Total_Images * image_size * 1e-12, # Convert to m^2
.groups = "drop"
)
list(particle_data = surface_particle_data,
summary_data = surface_summary_data,
trial_areas = trial_areas)
}
# Load before and after surface data
surface_before_data <- load_data("Bef")
surface_after_data <- load_data("Aft")
# Function to load and process calibration wafer data
load_calibration_data <- function(sample_number) {
# Format sample number with leading zeros
sample_str <- sprintf("%02d", sample_number)
# Construct file paths
particles_path <- sprintf("~/Desktop/College/Research/Dust_Contamination/Trials/Data/Calibration/Edge Measurements/Bef_%s minus Aft_%s/Particles_Reanalysis_Bef_%s minus Aft_%s.csv", sample_str, sample_str, sample_str, sample_str)
summary_path <- sprintf("~/Desktop/College/Research/Dust_Contamination/Trials/Data/Calibration/Edge Measurements/Bef_%s minus Aft_%s/Summary_Reanalysis_Bef_%s minus Aft_%s_updated.csv", sample_str, sample_str, sample_str, sample_str)
# Load particle data
particles_data <- read.csv(particles_path)
particles_data$Sample <- sample_number
# Load summary data
summary_data <- read.csv(summary_path)
names(summary_data) <- make.names(names(summary_data))
summary_data$Count <- as.numeric(gsub("[^0-9.]", "", summary_data$Count))
summary_data$width <- as.numeric(gsub("[^0-9.]", "", summary_data$width))
summary_data$Sample <- sample_number
summary_data <- summary_data %>% drop_na()
list(particles = particles_data, summary = summary_data)
}
# Load data for calibration samples (11-25)
calibration_data <- map(c(11:30), load_calibration_data)
# Function to load and process calibration wafer data
load_calibration_data <- function(sample_number) {
# Format sample number with leading zeros
sample_str <- sprintf("%02d", sample_number)
# Construct file paths
particles_path <- sprintf("~/Desktop/College/Research/Dust_Contamination/Trials/Data/Calibration/Edge Measurements/Bef_%s minus Aft_%s/Particles_Reanalysis_Bef_%s minus Aft_%s.csv", sample_str, sample_str, sample_str, sample_str)
summary_path <- sprintf("~/Desktop/College/Research/Dust_Contamination/Trials/Data/Calibration/Edge Measurements/Bef_%s minus Aft_%s/Summary_Reanalysis_Bef_%s minus Aft_%s_updated.csv", sample_str, sample_str, sample_str, sample_str)
# Load particle data
particles_data <- read.csv(particles_path)
particles_data$Sample <- sample_number
# Load summary data
summary_data <- read.csv(summary_path)
names(summary_data) <- make.names(names(summary_data))
summary_data$Count <- as.numeric(gsub("[^0-9.]", "", summary_data$Count))
summary_data$width <- as.numeric(gsub("[^0-9.]", "", summary_data$width))
summary_data$Sample <- sample_number
summary_data <- summary_data %>% drop_na()
list(particles = particles_data, summary = summary_data)
}
# Load data for calibration samples (11-25)
calibration_data <- map(c(11:30), load_calibration_data)
# Combine all particle data
calibration_particles_data <- bind_rows(map(calibration_data, "particles"))
# Combine all summary data
calibration_summary_data <- bind_rows(map(calibration_data, "summary"))
# Assign trials to samples
calibration_particles_data$Trial <- ceiling((calibration_particles_data$Sample - 5) / 5) + 1
calibration_summary_data$Trial <- ceiling((calibration_summary_data$Sample - 5) / 5) + 1
# Calculate total width and normalization factors
total_width_by_trial <- calibration_summary_data %>%
group_by(Trial) %>%
summarise(Total_Width = sum(edge_width))
calibration_max_width <- max_width
calibration_normalization_factors <- calibration_max_width / total_width_by_trial$Total_Width
rm(calibration_data)
surface_before_particles <- surface_before_data$particle_data
surface_after_particles <- surface_after_data$particle_data
combined_surface_data <- bind_rows(surface_before_particles,surface_after_particles)
# Create diameter thresholds
area_thresholds <- seq(1, max(combined_surface_data$Area), by = 10)
# Define IEST standard parameters and calculate sample areas
slope <- -0.926
image_size <- 600 * 450
new_sample_areas <- surface_before_data$summary_data %>%
group_by(Sample, Trial, Cleaning_Method) %>%
summarise(
Total_Images = n(),
Total_Area = n() * image_size * 1e-12,  # Convert to m^2
.groups = "drop"
)
# Function to get area-based counts
get_area_counts <- function(data, normalization_factor) {
sapply(area_thresholds, function(x) {
sum(data$Area > x) * normalization_factor
})
}
# Calculate area-based counts for before and after data
surface_before_counts <- surface_before_data$particle_data %>%
group_by(Trial, Sample) %>%
group_modify(~ {
norm_factor <- 0.1 /  new_sample_areas$Total_Area[new_sample_areas$Sample == .y$Sample &  new_sample_areas$Trial == .y$Trial]
counts <- get_area_counts(.x, norm_factor)
tibble(
Area = area_thresholds,
Count = counts
)
})
surface_after_counts <- surface_after_data$particle_data %>%
group_by(Trial, Sample) %>%
group_modify(~ {
norm_factor <- 0.1 /  new_sample_areas$Total_Area[new_sample_areas$Sample == .y$Sample &  new_sample_areas$Trial == .y$Trial]
counts <- get_area_counts(.x, norm_factor)
tibble(
Area = area_thresholds,
Count = counts
)
})
# Calculate differences and averages
surface_count_diff <- surface_after_counts %>%
full_join(surface_before_counts,
by = c("Trial", "Sample", "Area"),
suffix = c("_After", "_Before")) %>%
mutate(Count_Diff = Count_After - Count_Before,
Positive_Diff = pmax(Count_Diff, 0))
# Calculate average counts by trial
average_surface_counts <- surface_count_diff %>%
group_by(Trial, Area) %>%
summarize(
Average_Count = mean(Positive_Diff, na.rm = TRUE),
.groups = "drop"
)
# Calculate best fit lines
surface_best_fits <- average_surface_counts %>%
group_by(Trial) %>%
filter(Average_Count > 0) %>%
mutate(
log_area = log10(Area),
log_count = log10(Average_Count)
) %>%
summarise(
slope = coef(lm(log_count ~ log_area))[2],
intercept = coef(lm(log_count ~ log_area))[1],
.groups = "drop"
) %>%
mutate(PCL = 10^(-intercept/slope))
# Create visualization
ggplot() +
geom_line(data = average_surface_counts,
aes(x = log10(Area), y = log10(Average_Count),
color = factor(Trial))) +
geom_abline(data = surface_best_fits,
aes(slope = slope, intercept = intercept,
color = factor(Trial)),
linetype = "dotted") +
scale_color_manual(values = trial_colors, name = "Trial") +
labs(
x = expression(log[10](Area)~"(square microns)"),
y = "log10(Particle Count)",
title = "Surface Analysis: Particle Area Distribution",
subtitle = "Solid lines: observed data, Dotted lines: fitted curves"
) +
custom_theme
# Display PCL and slope statistics
kable(surface_best_fits,
caption = "PCL and Slope Statistics for Surface Analysis")
View(calibration_particles_data)
new_edge_factors<- edge_summary_data %>%
group_by(Sample, Trial) %>%
summarise(
Total_Width = sum(width),
.groups = "drop"
)
max_width <- max(new_edge_factors$Total_Width)
new_edge_factors <- new_edge_factors %>%
mutate(Factors = max_width/Total_Width)
# Calculate area-based counts for edge data
edge_counts <- edge_particles_data %>%
group_by(Trial, Sample) %>%
group_modify(~ {
norm_factor <- new_edge_factors$Factors[new_edge_factors$Sample == .y$Sample & new_edge_factors$Trial == .y$Trial]
counts <- get_area_counts(.x, norm_factor)
tibble(
Area = area_thresholds,
Count = counts
)
})
# Calculate cumulative counts
cumulative_edge_counts <- edge_counts %>%
group_by(Trial, Sample) %>%
arrange(desc(Area)) %>%
mutate(Cumulative_Count = cumsum(Count)) %>%
ungroup()
# Calculate average counts by trial
average_edge_counts <- cumulative_edge_counts %>%
group_by(Trial, Area) %>%
summarize(
Average_Count = mean(Cumulative_Count, na.rm = TRUE),
.groups = "drop"
) %>%
arrange(Trial, desc(Area))
# Calculate best fit lines
edge_best_fits <- average_edge_counts %>%
group_by(Trial) %>%
filter(Average_Count > 0) %>%
mutate(
log_area = log10(Area),
log_count = log10(Average_Count)
) %>%
summarise(
slope = coef(lm(log_count ~ log_area))[2],
intercept = coef(lm(log_count ~ log_area))[1],
.groups = "drop"
) %>%
mutate(PCL = 10^(-intercept/slope))
# Create edge distribution plot
ggplot() +
geom_line(data = average_edge_counts,
aes(x = log10(Area), y = log10(Average_Count),
color = factor(Trial))) +
geom_abline(data = edge_best_fits,
aes(slope = slope, intercept = intercept,
color = factor(Trial)),
linetype = "dotted") +
scale_color_manual(values = trial_colors, name = "Trial") +
labs(
x = expression(log[10](Area)~"(square microns)"),
y = "log10(Particle Count)",
title = "Edge Analysis: Particle Area Distribution",
subtitle = "Solid lines: observed data, Dotted lines: fitted curves"
) +
custom_theme
# Display edge analysis statistics
kable(edge_best_fits %>%
select(Trial, PCL, slope),
caption = "Edge Analysis Summary Statistics")
new_calibration_factors<- calibration_summary_data %>%
group_by(Sample, Trial) %>%
summarise(
Total_Width = sum(edge_width),
.groups = "drop"
)
max_width <- max(new_edge_factors$Total_Width)
new_calibration_factors <- new_calibration_factors %>%
mutate(Factors = max_width/Total_Width)
calibration_summary_data <- calibration_summary_data %>%
group_by(Sample, Trial) %>%
mutate(
Image = row_number(),
CumulativeCount = cumsum(Count)
) %>%
ungroup()
calibration_particles_data <- calibration_particles_data %>%
group_by(Sample, Trial) %>%
mutate(
ParticleIndex = row_number(),
Image = findInterval(ParticleIndex, calibration_summary_data$CumulativeCount[calibration_summary_data$Sample == first(Sample) & calibration_summary_data$Trial == first(Trial)]) + 1
) %>%
ungroup()
# Calculate area-based counts for calibration data
calibration_counts <- calibration_particles_data %>%
group_by(Trial, Sample) %>%
group_modify(~ {
norm_factor <- new_calibration_factors$Factors[new_calibration_factors$Sample == .y$Sample & new_calibration_factors$Trial == .y$Trial]
counts <- get_area_counts(.x, norm_factor)
tibble(
Area = area_thresholds,
Count = counts
)
})
# Calculate cumulative counts
cumulative_calibration_counts <- calibration_counts %>%
group_by(Trial, Sample) %>%
arrange(desc(Area)) %>%
mutate(Cumulative_Count = cumsum(Count)) %>%
ungroup()
# Calculate average counts by trial
average_calibration_counts <- cumulative_calibration_counts %>%
group_by(Trial, Area) %>%
summarize(
Average_Count = mean(Cumulative_Count, na.rm = TRUE),
.groups = "drop"
) %>%
mutate(Trial = Trial + 5) %>%  # Adjust trial numbers to match other analyses
arrange(Trial, desc(Area))
# Calculate best fit lines
calibration_best_fits <- average_calibration_counts %>%
group_by(Trial) %>%
filter(Average_Count > 0) %>%
mutate(
log_area = log10(Area),
log_count = log10(Average_Count)
) %>%
summarise(
slope = coef(lm(log_count ~ log_area))[2],
intercept = coef(lm(log_count ~ log_area))[1],
.groups = "drop"
) %>%
mutate(PCL = 10^(-intercept/slope))
# Create calibration visualization
ggplot() +
geom_line(data = average_calibration_counts,
aes(x = log10(Area), y = log10(Average_Count),
color = factor(Trial))) +
geom_abline(data = calibration_best_fits,
aes(slope = slope, intercept = intercept,
color = factor(Trial)),
linetype = "dotted") +
scale_color_manual(values = trial_colors, name = "Trial") +
labs(
x = expression(log[10](Area)~"(square microns)"),
y = "log10(Particle Count)",
title = "Calibration Wafer Analysis: Particle Area Distribution",
subtitle = "Solid lines: observed data, Dotted lines: fitted curves"
) +
custom_theme
# Display calibration analysis statistics
kable(calibration_best_fits,
caption = "Calibration Wafer Analysis Summary Statistics")
View(calibration_counts)
View(new_calibration_factors)
View(total_width_by_trial)
View(calibration_summary_data)
